{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTze-VbeU1c0"
   },
   "source": [
    "# Fine-tune a DialoGPT model\n",
    "\n",
    "Adapted from the notebook in [this Medium post](https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30?gi=e4a72d1510f0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y17kuzFNUSrZ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBfltjGHT6KG",
    "outputId": "7822e15b-9c77-412a-a6ed-20100243db13"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8fgmjaqUErq"
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtCreyG8UG1s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:38:09.894395Z",
     "iopub.status.busy": "2022-05-14T01:38:09.893703Z",
     "iopub.status.idle": "2022-05-14T01:38:12.791162Z",
     "shell.execute_reply": "2022-05-14T01:38:12.790708Z",
     "shell.execute_reply.started": "2022-05-14T01:38:09.894298Z"
    },
    "id": "dnv5kT-mLsB-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all the imports\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmrbGB8aUmBm"
   },
   "source": [
    "## Get Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftBYBoOoV_Er",
    "outputId": "07da0a13-6112-4c4e-cb49-51580c2d9e7a"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbITTMcLVbI_",
    "outputId": "fb4c8bf1-ff2d-4952-a451-62cdd0655aea"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download ruolinzheng/twewy-game-script -f twewy-name-line-full.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:38:21.288729Z",
     "iopub.status.busy": "2022-05-14T01:38:21.288201Z",
     "iopub.status.idle": "2022-05-14T01:38:21.393512Z",
     "shell.execute_reply": "2022-05-14T01:38:21.393041Z",
     "shell.execute_reply.started": "2022-05-14T01:38:21.288694Z"
    },
    "id": "RXdJTSVwWGHj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "execution": {
     "iopub.execute_input": "2022-05-14T01:38:22.916139Z",
     "iopub.status.busy": "2022-05-14T01:38:22.915737Z",
     "iopub.status.idle": "2022-05-14T01:38:22.942308Z",
     "shell.execute_reply": "2022-05-14T01:38:22.941959Z",
     "shell.execute_reply.started": "2022-05-14T01:38:22.916115Z"
    },
    "id": "h6kGx-9eG7qA",
    "outputId": "bd2efe43-1e50-4716-81a2-bf15a3dd03bd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>ms. ellen</td>\n",
       "      <td>that's very nice mr chef now if you're finished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29523</th>\n",
       "      <td>cartman</td>\n",
       "      <td>we've only got one option you guys we're gonna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56646</th>\n",
       "      <td>larry king</td>\n",
       "      <td>so stan what motivated you what inspired you t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57509</th>\n",
       "      <td>chairman</td>\n",
       "      <td>ladies and gentlemen for the past several days...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12791</th>\n",
       "      <td>ned</td>\n",
       "      <td>i lost my voice box</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>mecha-streisand</td>\n",
       "      <td>mecha streisand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                               line\n",
       "3639         ms. ellen    that's very nice mr chef now if you're finished\n",
       "29523          cartman  we've only got one option you guys we're gonna...\n",
       "56646       larry king  so stan what motivated you what inspired you t...\n",
       "57509         chairman  ladies and gentlemen for the past several days...\n",
       "12791              ned                                i lost my voice box\n",
       "3911   mecha-streisand                                    mecha streisand"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:38:24.626449Z",
     "iopub.status.busy": "2022-05-14T01:38:24.625907Z",
     "iopub.status.idle": "2022-05-14T01:38:24.630563Z",
     "shell.execute_reply": "2022-05-14T01:38:24.629739Z",
     "shell.execute_reply.started": "2022-05-14T01:38:24.626403Z"
    },
    "id": "PG8v6--qWUwj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHARACTER_NAME = 'cartman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:39:10.444713Z",
     "iopub.status.busy": "2022-05-14T01:39:10.444055Z",
     "iopub.status.idle": "2022-05-14T01:39:11.183974Z",
     "shell.execute_reply": "2022-05-14T01:39:11.183495Z",
     "shell.execute_reply.started": "2022-05-14T01:39:10.444670Z"
    },
    "id": "GZUcEMd2WLDT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contexted = []\n",
    "\n",
    "# context window of size 7\n",
    "n = 7\n",
    "\n",
    "for i in data[data.name == CHARACTER_NAME].index:\n",
    "    if i < n:\n",
    "        continue\n",
    "    row = []\n",
    "    prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
    "    for j in range(i, prev, -1):\n",
    "        row.append(data.line[j])\n",
    "        contexted.append(row)\n",
    "\n",
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/' + str(i) for i in range(n - 1)]\n",
    "\n",
    "df = pd.DataFrame.from_records(contexted, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "execution": {
     "iopub.execute_input": "2022-05-14T01:39:18.225357Z",
     "iopub.status.busy": "2022-05-14T01:39:18.225068Z",
     "iopub.status.idle": "2022-05-14T01:39:18.238854Z",
     "shell.execute_reply": "2022-05-14T01:39:18.238318Z",
     "shell.execute_reply.started": "2022-05-14T01:39:18.225329Z"
    },
    "id": "4T5OlNZHUxij",
    "outputId": "895603a6-ca02-4301-c4b0-5bccbee8a3b8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9683</th>\n",
       "      <td>hey where did you guys win all those terrance ...</td>\n",
       "      <td>well i dreamt that i was a poor vietnamese gir...</td>\n",
       "      <td>really what about</td>\n",
       "      <td>oh pretty good except i had the weirdest dream...</td>\n",
       "      <td>oh hey cartman how are you feeling</td>\n",
       "      <td>what are you guys doing</td>\n",
       "      <td>oh good idea phillip let's look for treasuh</td>\n",
       "      <td>say terrance let's look for treasuh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76107</th>\n",
       "      <td>what did you say</td>\n",
       "      <td>we took all kinds of video with his drone</td>\n",
       "      <td>i'm dying of boredom</td>\n",
       "      <td>you can see the momma bird fly in</td>\n",
       "      <td>that's stupid</td>\n",
       "      <td>my dad and i shot it yesterday at the lake</td>\n",
       "      <td>no it isn't</td>\n",
       "      <td>i don't know this bird nest is pretty neato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>hey the white man has marred my people long en...</td>\n",
       "      <td>what</td>\n",
       "      <td>native american stan and the bear is very impo...</td>\n",
       "      <td>cartman what the hell are you doing dressed up...</td>\n",
       "      <td>yeah he's almost as bad as that guy on full house</td>\n",
       "      <td>this guy sucks</td>\n",
       "      <td>bob saget</td>\n",
       "      <td>bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53983</th>\n",
       "      <td>excuse me excuse me i need to go i gotta run</td>\n",
       "      <td>i didn't say that yes i did but why oh stop it</td>\n",
       "      <td>you you and your cousin touched wieners</td>\n",
       "      <td>uh my cousin my cousin one time my cousin and ...</td>\n",
       "      <td>wuh who are you talking about</td>\n",
       "      <td>butters do you think it's possible that you ca...</td>\n",
       "      <td>uh hey eric</td>\n",
       "      <td>uhuh excuse me i i need a toilet bathroom bath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79502</th>\n",
       "      <td>that was nice</td>\n",
       "      <td>troll safe doors my safe spaaaaaaaace</td>\n",
       "      <td>bully proof windows</td>\n",
       "      <td>nooooo</td>\n",
       "      <td>but reality we can do without</td>\n",
       "      <td>we can face almost anything</td>\n",
       "      <td>drat</td>\n",
       "      <td>our safe spaces will keep you out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14658</th>\n",
       "      <td>just hear those sleigh bells jingle in' ring t...</td>\n",
       "      <td>this is gonna be fun</td>\n",
       "      <td>relax dude what's the big deal think about it ...</td>\n",
       "      <td>naww let's dig up somebody else</td>\n",
       "      <td>all we have to do is sneak in the graveyard di...</td>\n",
       "      <td>dude we're not digging up my grandma i'll get ...</td>\n",
       "      <td>yeah</td>\n",
       "      <td>u hi think that's a sweet idea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response  \\\n",
       "9683   hey where did you guys win all those terrance ...   \n",
       "76107                                   what did you say   \n",
       "5121   hey the white man has marred my people long en...   \n",
       "53983       excuse me excuse me i need to go i gotta run   \n",
       "79502                                      that was nice   \n",
       "14658  just hear those sleigh bells jingle in' ring t...   \n",
       "\n",
       "                                                 context  \\\n",
       "9683   well i dreamt that i was a poor vietnamese gir...   \n",
       "76107          we took all kinds of video with his drone   \n",
       "5121                                                what   \n",
       "53983     i didn't say that yes i did but why oh stop it   \n",
       "79502              troll safe doors my safe spaaaaaaaace   \n",
       "14658                               this is gonna be fun   \n",
       "\n",
       "                                               context/0  \\\n",
       "9683                                   really what about   \n",
       "76107                               i'm dying of boredom   \n",
       "5121   native american stan and the bear is very impo...   \n",
       "53983            you you and your cousin touched wieners   \n",
       "79502                                bully proof windows   \n",
       "14658  relax dude what's the big deal think about it ...   \n",
       "\n",
       "                                               context/1  \\\n",
       "9683   oh pretty good except i had the weirdest dream...   \n",
       "76107                  you can see the momma bird fly in   \n",
       "5121   cartman what the hell are you doing dressed up...   \n",
       "53983  uh my cousin my cousin one time my cousin and ...   \n",
       "79502                                             nooooo   \n",
       "14658                    naww let's dig up somebody else   \n",
       "\n",
       "                                               context/2  \\\n",
       "9683                  oh hey cartman how are you feeling   \n",
       "76107                                      that's stupid   \n",
       "5121   yeah he's almost as bad as that guy on full house   \n",
       "53983                      wuh who are you talking about   \n",
       "79502                      but reality we can do without   \n",
       "14658  all we have to do is sneak in the graveyard di...   \n",
       "\n",
       "                                               context/3  \\\n",
       "9683                             what are you guys doing   \n",
       "76107         my dad and i shot it yesterday at the lake   \n",
       "5121                                      this guy sucks   \n",
       "53983  butters do you think it's possible that you ca...   \n",
       "79502                        we can face almost anything   \n",
       "14658  dude we're not digging up my grandma i'll get ...   \n",
       "\n",
       "                                         context/4  \\\n",
       "9683   oh good idea phillip let's look for treasuh   \n",
       "76107                                  no it isn't   \n",
       "5121                                     bob saget   \n",
       "53983                                  uh hey eric   \n",
       "79502                                         drat   \n",
       "14658                                         yeah   \n",
       "\n",
       "                                               context/5  \n",
       "9683                 say terrance let's look for treasuh  \n",
       "76107        i don't know this bird nest is pretty neato  \n",
       "5121                                                 bob  \n",
       "53983  uhuh excuse me i i need a toilet bathroom bath...  \n",
       "79502                  our safe spaces will keep you out  \n",
       "14658                     u hi think that's a sweet idea  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "execution": {
     "iopub.execute_input": "2022-05-14T01:40:12.935093Z",
     "iopub.status.busy": "2022-05-14T01:40:12.934867Z",
     "iopub.status.idle": "2022-05-14T01:40:12.964625Z",
     "shell.execute_reply": "2022-05-14T01:40:12.963995Z",
     "shell.execute_reply.started": "2022-05-14T01:40:12.935076Z"
    },
    "id": "NGy0MxMQVIAP",
    "outputId": "08b7f0eb-6a38-4b83-efdc-e53778d7547a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23802</th>\n",
       "      <td>wow this is gonna be great a whole new word</td>\n",
       "      <td>yes that's now fine</td>\n",
       "      <td>uh ms choksondik eh can we say it in the exple...</td>\n",
       "      <td>very good timmy</td>\n",
       "      <td>sss sss shit</td>\n",
       "      <td>the adjective form is now also acceptable for ...</td>\n",
       "      <td>me neither</td>\n",
       "      <td>i don't get it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9060</th>\n",
       "      <td>who the hell are you</td>\n",
       "      <td>oh hey scott</td>\n",
       "      <td>what's up</td>\n",
       "      <td>hey girls</td>\n",
       "      <td>coming mom okay we have to play truth or dare ...</td>\n",
       "      <td>eric are you out there in your clubhouse</td>\n",
       "      <td>eric snookums it's time for mommy to tuck you ...</td>\n",
       "      <td>totally uuuuuh my mom gives me shit sometimes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78032</th>\n",
       "      <td>he's such a douchebag</td>\n",
       "      <td>please tune in and help get america's families...</td>\n",
       "      <td>there's something for every generation to brin...</td>\n",
       "      <td>cartmaaanbrah dude check it out lady gaga is s...</td>\n",
       "      <td>and featuring live commentary the entire show ...</td>\n",
       "      <td>i'm flying hee dah dada dat dih</td>\n",
       "      <td>from the producers of women of rock live and t...</td>\n",
       "      <td>no it's great it's just what this country need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49015</th>\n",
       "      <td>well yeah</td>\n",
       "      <td>is this about more than your video game are yo...</td>\n",
       "      <td>because you took my xbox</td>\n",
       "      <td>eric can i just talk to you for a quick second...</td>\n",
       "      <td>don't stop believing hold on to that feeling</td>\n",
       "      <td>all right i've dealt with this before we just ...</td>\n",
       "      <td>yeah it's best to avoid his level</td>\n",
       "      <td>well then i'll just have to stand right here o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61903</th>\n",
       "      <td>oh god damn it son of a bitch idea stealing as...</td>\n",
       "      <td>theh they can't do that i'm student body presi...</td>\n",
       "      <td>james cameron go look the movie came out already</td>\n",
       "      <td>you sold the movie rights to who</td>\n",
       "      <td>luckily with the money i made selling the movi...</td>\n",
       "      <td>no no dude you don't just take one person's st...</td>\n",
       "      <td>no you're right the students all deserve to kn...</td>\n",
       "      <td>dude you can't take my dances with smurfs idea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response  \\\n",
       "23802        wow this is gonna be great a whole new word   \n",
       "9060                                who the hell are you   \n",
       "78032                              he's such a douchebag   \n",
       "49015                                          well yeah   \n",
       "61903  oh god damn it son of a bitch idea stealing as...   \n",
       "\n",
       "                                                 context  \\\n",
       "23802                                yes that's now fine   \n",
       "9060                                        oh hey scott   \n",
       "78032  please tune in and help get america's families...   \n",
       "49015  is this about more than your video game are yo...   \n",
       "61903  theh they can't do that i'm student body presi...   \n",
       "\n",
       "                                               context/0  \\\n",
       "23802  uh ms choksondik eh can we say it in the exple...   \n",
       "9060                                           what's up   \n",
       "78032  there's something for every generation to brin...   \n",
       "49015                           because you took my xbox   \n",
       "61903   james cameron go look the movie came out already   \n",
       "\n",
       "                                               context/1  \\\n",
       "23802                                    very good timmy   \n",
       "9060                                           hey girls   \n",
       "78032  cartmaaanbrah dude check it out lady gaga is s...   \n",
       "49015  eric can i just talk to you for a quick second...   \n",
       "61903                   you sold the movie rights to who   \n",
       "\n",
       "                                               context/2  \\\n",
       "23802                                       sss sss shit   \n",
       "9060   coming mom okay we have to play truth or dare ...   \n",
       "78032  and featuring live commentary the entire show ...   \n",
       "49015       don't stop believing hold on to that feeling   \n",
       "61903  luckily with the money i made selling the movi...   \n",
       "\n",
       "                                               context/3  \\\n",
       "23802  the adjective form is now also acceptable for ...   \n",
       "9060            eric are you out there in your clubhouse   \n",
       "78032                    i'm flying hee dah dada dat dih   \n",
       "49015  all right i've dealt with this before we just ...   \n",
       "61903  no no dude you don't just take one person's st...   \n",
       "\n",
       "                                               context/4  \\\n",
       "23802                                         me neither   \n",
       "9060   eric snookums it's time for mommy to tuck you ...   \n",
       "78032  from the producers of women of rock live and t...   \n",
       "49015                  yeah it's best to avoid his level   \n",
       "61903  no you're right the students all deserve to kn...   \n",
       "\n",
       "                                               context/5  \n",
       "23802                                     i don't get it  \n",
       "9060   totally uuuuuh my mom gives me shit sometimes ...  \n",
       "78032  no it's great it's just what this country need...  \n",
       "49015  well then i'll just have to stand right here o...  \n",
       "61903  dude you can't take my dances with smurfs idea...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size=0.1)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:40:50.288597Z",
     "iopub.status.busy": "2022-05-14T01:40:50.287971Z",
     "iopub.status.idle": "2022-05-14T01:40:50.301589Z",
     "shell.execute_reply": "2022-05-14T01:40:50.300668Z",
     "shell.execute_reply.started": "2022-05-14T01:40:50.288554Z"
    },
    "id": "aEeJQlAKWtiJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dataset suitable for our model\n",
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:41:04.618169Z",
     "iopub.status.busy": "2022-05-14T01:41:04.617713Z",
     "iopub.status.idle": "2022-05-14T01:41:04.629415Z",
     "shell.execute_reply": "2022-05-14T01:41:04.628737Z",
     "shell.execute_reply.started": "2022-05-14T01:41:04.618136Z"
    },
    "id": "-3iHwoKlWyrs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cacheing and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEDdTJTqUwZJ"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-05-14T01:41:21.669456Z",
     "iopub.status.busy": "2022-05-14T01:41:21.668985Z",
     "iopub.status.idle": "2022-05-14T01:41:28.258079Z",
     "shell.execute_reply": "2022-05-14T01:41:28.257581Z",
     "shell.execute_reply.started": "2022-05-14T01:41:21.669423Z"
    },
    "id": "r2cE0fY5UHpz",
    "outputId": "e4f382cd-57d9-49b7-9da4-4b44fe57df5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:41:43.029637Z",
     "iopub.status.busy": "2022-05-14T01:41:43.029109Z",
     "iopub.status.idle": "2022-05-14T01:41:43.047250Z",
     "shell.execute_reply": "2022-05-14T01:41:43.046664Z",
     "shell.execute_reply.started": "2022-05-14T01:41:43.029605Z"
    },
    "id": "ra2vsRp-UMXo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:41:56.990372Z",
     "iopub.status.busy": "2022-05-14T01:41:56.989873Z",
     "iopub.status.idle": "2022-05-14T01:41:56.998321Z",
     "shell.execute_reply": "2022-05-14T01:41:56.997665Z",
     "shell.execute_reply.started": "2022-05-14T01:41:56.990339Z"
    },
    "id": "2OnASqJjUNJa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output-small'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 4\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q1dTFXxW9NE"
   },
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:42:15.444503Z",
     "iopub.status.busy": "2022-05-14T01:42:15.444226Z",
     "iopub.status.idle": "2022-05-14T01:42:15.491258Z",
     "shell.execute_reply": "2022-05-14T01:42:15.490530Z",
     "shell.execute_reply.started": "2022-05-14T01:42:15.444471Z"
    },
    "id": "PaarIDZrW81h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:42:27.712617Z",
     "iopub.status.busy": "2022-05-14T01:42:27.712348Z",
     "iopub.status.idle": "2022-05-14T01:42:27.731173Z",
     "shell.execute_reply": "2022-05-14T01:42:27.730563Z",
     "shell.execute_reply.started": "2022-05-14T01:42:27.712585Z"
    },
    "id": "SCnGAJWbXD9C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NWvkdR-XHeB"
   },
   "source": [
    "## Run the Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780,
     "referenced_widgets": [
      "1d7f4c82687540f1ad69eb54ac3c25b4",
      "e7b9f3fc77a24259a87ef0dc735dfecb",
      "f3bf54733c2d4d9daa1cc9a7746ccb14",
      "aa40eb6346b54e7dac98e0b068cd4927",
      "021b771a270f479aa3b9e2b5f17e3d97",
      "450b0e7fd7a347c7beb78b7d72f64385",
      "9391d7abf6ed4400903995f56d7a1260",
      "ea6b919964d24c2f9de1c64c9cefaf23",
      "2fa1fa2407384cb98d79a912de2d5b8f",
      "dc27e2caf1ea4a4ab9ae3708fb06952f",
      "e38fb98fd7b3413392dc39c93a107a35",
      "855ca0a6125a4d698416214a9425ad98",
      "4699416338ae40a5b6abf19e45089aec",
      "43fdb31d3f314624ba07a15718b0c8f3",
      "de252cd193114c40ad5f5e9622b7abc7",
      "5e48b617cc3f41c3945efc28fc5e0c75",
      "68a9dc52819c48fb97259f318f9b5c6a",
      "b4e00059cf3a49929978ed780aae8358",
      "0ff5f4e3506b493a98d72008a467f35f",
      "77b97fa3271b48ac9f93665a102b4fd1",
      "a937f1dfeee5432ba31b3016fd30e9e2",
      "3c6d446f491c48fcae03e0034bfaaae9",
      "a193bb3a0b5b4cbba587e2460075a445",
      "75f8aebc30304fe198b5a2898a53a92d",
      "8b8a7c771d234f6c9d758a1f07f75a90",
      "c6518c4a721745bf97ee682f2ebe4635",
      "29cffa2b4f234e12802344eb53838641",
      "96243b7b227f465f83a289481680b925",
      "8c016a54f0a24fcdacf369baa9d24f1e",
      "7fe5b457ca0f417f90a20d235e9cec07",
      "fdffb26b99c24c978580f1cf97359fea",
      "8e3f1740c82f47949eefc2eb53052eae",
      "9cccd43f6acc4e25b4876fd0ae7a2ad6",
      "175e94deab7f4d20b99b419bea33583b",
      "41f26f7210e540479814e5d68de13ddb",
      "cf5cd281fa3b453093e210650bf81e9e",
      "e1fbe239c2394cbf973ac5b95e1e1491",
      "810ac22adad344b7bf8b556ded990122",
      "8b3a41c1900b45ebb9c56601deca0e84",
      "002f56aac3d64b33a0e799c0baf1e6b9",
      "a0f2a9a279734aa5bf146f0a5b33c43b",
      "850b5411122e4d608511fe26818bea68",
      "0663fb4bd85f4d87a7d61910b995be14",
      "cb7f52610fcf49bda46a14b296ff5bb5",
      "0ca29b4a62e04d9c937189ea19b25de8",
      "f871b83632974e0088bae65e78efaf28",
      "4cacf7fc20754a7ca7fe08c8ec187a81",
      "8bcc625c0f284398bbd287fe45021b17"
     ]
    },
    "execution": {
     "iopub.execute_input": "2022-05-14T01:42:33.657291Z",
     "iopub.status.busy": "2022-05-14T01:42:33.656861Z",
     "iopub.status.idle": "2022-05-14T01:42:42.441765Z",
     "shell.execute_reply": "2022-05-14T01:42:42.440823Z",
     "shell.execute_reply.started": "2022-05-14T01:42:33.657262Z"
    },
    "id": "e61zo2JtXGNX",
    "outputId": "22d4916e-7169-44b5-f9d8-79b9c43fab2e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2022 21:42:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/13/2022 21:42:41 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f718db90460>\n",
      "05/13/2022 21:42:41 - INFO - __main__ -   Creating features from dataset file at cached\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(df_trn, df_val)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[0;32m---> 63\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     global_step, tr_loss \u001b[38;5;241m=\u001b[39m train(args, train_dataset, model, tokenizer)\n\u001b[1;32m     66\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m global_step = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, average loss = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, global_step, tr_loss)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mload_and_cache_examples\u001b[0;34m(args, tokenizer, df_trn, df_val, evaluate)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_cache_examples\u001b[39m(args, tokenizer, df_trn, df_val, evaluate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConversationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_trn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mConversationDataset.__init__\u001b[0;34m(self, tokenizer, args, df, block_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 27\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples\u001b[38;5;241m.\u001b[39mappend(conv)\n\u001b[1;32m     30\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving features into cached file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cached_features_file)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mconstruct_conv\u001b[0;34m(row, tokenizer, eos)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_conv\u001b[39m(row, tokenizer, eos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m     flatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m l \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m----> 4\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m([tokenizer\u001b[38;5;241m.\u001b[39mencode(x) \u001b[38;5;241m+\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39meos_token_id] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m row]))\n\u001b[1;32m      5\u001b[0m     conv \u001b[38;5;241m=\u001b[39m flatten(conv)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conv\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_conv\u001b[39m(row, tokenizer, eos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m     flatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m l \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m----> 4\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m([\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39meos_token_id] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m row]))\n\u001b[1;32m      5\u001b[0m     conv \u001b[38;5;241m=\u001b[39m flatten(conv)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conv\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2240\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2204\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2224\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2225\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2240\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2570\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2561\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2562\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2563\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2568\u001b[0m )\n\u001b[0;32m-> 2570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:174\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m )\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:498\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    477\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    495\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    497\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 498\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:164\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m )\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:425\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    418\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    419\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    423\u001b[0m )\n\u001b[0;32m--> 425\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    437\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    439\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    449\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRpQ_n2zXQj-"
   },
   "source": [
    "## Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGw3qgfaXQHX",
    "outputId": "93e84cfd-9718-42e5-bd11-418112c91d71"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('output-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAWsiAvNXbxd",
    "outputId": "0fd2541e-ee68-4976-b098-8483efe38d5e"
   },
   "outputs": [],
   "source": [
    "# Let's chat for 4 lines\n",
    "for step in range(4):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"JoshuaBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANSQlQezXqwn"
   },
   "source": [
    "## Push Model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgnHRgHKXwDd"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhqMtvfmXei8"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"lynnzheng08@outlook.com\"\n",
    "# Tip: using the same email as your huggingface.co account will link your commits to your profile\n",
    "!git config --global user.name \"Lynn Zheng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfUsrKR7YLT1"
   },
   "outputs": [],
   "source": [
    "MY_MODEL_NAME = 'DialoGPT-small-joshua'\n",
    "with open('HuggingFace-API-key.txt', 'rt') as f:\n",
    "  HUGGINGFACE_API_KEY = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_65nsiLcYNXI",
    "outputId": "0dbf0cb1-957c-4adb-bf55-4222d2cc85bc"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(MY_MODEL_NAME, use_auth_token=HUGGINGFACE_API_KEY)\n",
    "tokenizer.push_to_hub(MY_MODEL_NAME, use_auth_token=HUGGINGFACE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_XfXTCrZKmO"
   },
   "source": [
    "## All Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tIwK7G8ZLrd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_train_upload_workflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002f56aac3d64b33a0e799c0baf1e6b9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "021b771a270f479aa3b9e2b5f17e3d97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0663fb4bd85f4d87a7d61910b995be14": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Evaluating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f871b83632974e0088bae65e78efaf28",
      "max": 21,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ca29b4a62e04d9c937189ea19b25de8",
      "value": 21
     }
    },
    "0ca29b4a62e04d9c937189ea19b25de8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0ff5f4e3506b493a98d72008a467f35f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c6d446f491c48fcae03e0034bfaaae9",
      "max": 195,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a937f1dfeee5432ba31b3016fd30e9e2",
      "value": 195
     }
    },
    "175e94deab7f4d20b99b419bea33583b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d7f4c82687540f1ad69eb54ac3c25b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3bf54733c2d4d9daa1cc9a7746ccb14",
       "IPY_MODEL_aa40eb6346b54e7dac98e0b068cd4927"
      ],
      "layout": "IPY_MODEL_e7b9f3fc77a24259a87ef0dc735dfecb"
     }
    },
    "29cffa2b4f234e12802344eb53838641": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fe5b457ca0f417f90a20d235e9cec07",
      "max": 195,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c016a54f0a24fcdacf369baa9d24f1e",
      "value": 195
     }
    },
    "2fa1fa2407384cb98d79a912de2d5b8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e38fb98fd7b3413392dc39c93a107a35",
       "IPY_MODEL_855ca0a6125a4d698416214a9425ad98"
      ],
      "layout": "IPY_MODEL_dc27e2caf1ea4a4ab9ae3708fb06952f"
     }
    },
    "3c6d446f491c48fcae03e0034bfaaae9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41f26f7210e540479814e5d68de13ddb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_810ac22adad344b7bf8b556ded990122",
      "max": 195,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1fbe239c2394cbf973ac5b95e1e1491",
      "value": 195
     }
    },
    "43fdb31d3f314624ba07a15718b0c8f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "450b0e7fd7a347c7beb78b7d72f64385": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4699416338ae40a5b6abf19e45089aec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4cacf7fc20754a7ca7fe08c8ec187a81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e48b617cc3f41c3945efc28fc5e0c75": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68a9dc52819c48fb97259f318f9b5c6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ff5f4e3506b493a98d72008a467f35f",
       "IPY_MODEL_77b97fa3271b48ac9f93665a102b4fd1"
      ],
      "layout": "IPY_MODEL_b4e00059cf3a49929978ed780aae8358"
     }
    },
    "75f8aebc30304fe198b5a2898a53a92d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77b97fa3271b48ac9f93665a102b4fd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75f8aebc30304fe198b5a2898a53a92d",
      "placeholder": "​",
      "style": "IPY_MODEL_a193bb3a0b5b4cbba587e2460075a445",
      "value": " 195/195 [00:35&lt;00:00,  5.45it/s]"
     }
    },
    "7fe5b457ca0f417f90a20d235e9cec07": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "810ac22adad344b7bf8b556ded990122": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "850b5411122e4d608511fe26818bea68": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "855ca0a6125a4d698416214a9425ad98": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e48b617cc3f41c3945efc28fc5e0c75",
      "placeholder": "​",
      "style": "IPY_MODEL_de252cd193114c40ad5f5e9622b7abc7",
      "value": " 195/195 [00:44&lt;00:00,  4.39it/s]"
     }
    },
    "8b3a41c1900b45ebb9c56601deca0e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b8a7c771d234f6c9d758a1f07f75a90": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_29cffa2b4f234e12802344eb53838641",
       "IPY_MODEL_96243b7b227f465f83a289481680b925"
      ],
      "layout": "IPY_MODEL_c6518c4a721745bf97ee682f2ebe4635"
     }
    },
    "8bcc625c0f284398bbd287fe45021b17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c016a54f0a24fcdacf369baa9d24f1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8e3f1740c82f47949eefc2eb53052eae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9391d7abf6ed4400903995f56d7a1260": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96243b7b227f465f83a289481680b925": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e3f1740c82f47949eefc2eb53052eae",
      "placeholder": "​",
      "style": "IPY_MODEL_fdffb26b99c24c978580f1cf97359fea",
      "value": " 195/195 [01:17&lt;00:00,  2.53it/s]"
     }
    },
    "9cccd43f6acc4e25b4876fd0ae7a2ad6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_41f26f7210e540479814e5d68de13ddb",
       "IPY_MODEL_cf5cd281fa3b453093e210650bf81e9e"
      ],
      "layout": "IPY_MODEL_175e94deab7f4d20b99b419bea33583b"
     }
    },
    "a0f2a9a279734aa5bf146f0a5b33c43b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0663fb4bd85f4d87a7d61910b995be14",
       "IPY_MODEL_cb7f52610fcf49bda46a14b296ff5bb5"
      ],
      "layout": "IPY_MODEL_850b5411122e4d608511fe26818bea68"
     }
    },
    "a193bb3a0b5b4cbba587e2460075a445": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a937f1dfeee5432ba31b3016fd30e9e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aa40eb6346b54e7dac98e0b068cd4927": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea6b919964d24c2f9de1c64c9cefaf23",
      "placeholder": "​",
      "style": "IPY_MODEL_9391d7abf6ed4400903995f56d7a1260",
      "value": " 4/4 [02:23&lt;00:00, 36.00s/it]"
     }
    },
    "b4e00059cf3a49929978ed780aae8358": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6518c4a721745bf97ee682f2ebe4635": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb7f52610fcf49bda46a14b296ff5bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bcc625c0f284398bbd287fe45021b17",
      "placeholder": "​",
      "style": "IPY_MODEL_4cacf7fc20754a7ca7fe08c8ec187a81",
      "value": " 21/21 [00:01&lt;00:00, 10.78it/s]"
     }
    },
    "cf5cd281fa3b453093e210650bf81e9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_002f56aac3d64b33a0e799c0baf1e6b9",
      "placeholder": "​",
      "style": "IPY_MODEL_8b3a41c1900b45ebb9c56601deca0e84",
      "value": " 195/195 [00:40&lt;00:00,  4.84it/s]"
     }
    },
    "dc27e2caf1ea4a4ab9ae3708fb06952f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de252cd193114c40ad5f5e9622b7abc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1fbe239c2394cbf973ac5b95e1e1491": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e38fb98fd7b3413392dc39c93a107a35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Iteration: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43fdb31d3f314624ba07a15718b0c8f3",
      "max": 195,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4699416338ae40a5b6abf19e45089aec",
      "value": 195
     }
    },
    "e7b9f3fc77a24259a87ef0dc735dfecb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea6b919964d24c2f9de1c64c9cefaf23": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3bf54733c2d4d9daa1cc9a7746ccb14": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_450b0e7fd7a347c7beb78b7d72f64385",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_021b771a270f479aa3b9e2b5f17e3d97",
      "value": 4
     }
    },
    "f871b83632974e0088bae65e78efaf28": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdffb26b99c24c978580f1cf97359fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
